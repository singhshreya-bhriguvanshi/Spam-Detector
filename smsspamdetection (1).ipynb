{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9440000,"sourceType":"datasetVersion","datasetId":5736412},{"sourceId":10077690,"sourceType":"datasetVersion","datasetId":6212282},{"sourceId":10184035,"sourceType":"datasetVersion","datasetId":6291261}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers datasets sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T06:46:32.058557Z","iopub.execute_input":"2024-12-17T06:46:32.058904Z","iopub.status.idle":"2024-12-17T06:50:25.696334Z","shell.execute_reply.started":"2024-12-17T06:46:32.058865Z","shell.execute_reply":"2024-12-17T06:50:25.695274Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79682368ca00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sacrebleu/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79682368cd00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sacrebleu/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79682368ceb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sacrebleu/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79682368d060>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sacrebleu/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79682368d210>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sacrebleu/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sacrebleu (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for sacrebleu\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# List the files in the directory\nmodel_path = \"/kaggle/input/pretrained-filess/\"\nfiles = os.listdir(model_path)\nprint(files)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:09:57.789129Z","iopub.execute_input":"2024-12-17T05:09:57.789893Z","iopub.status.idle":"2024-12-17T05:09:57.794601Z","shell.execute_reply.started":"2024-12-17T05:09:57.789859Z","shell.execute_reply":"2024-12-17T05:09:57.793841Z"}},"outputs":[{"name":"stdout","text":"['config.json', 'tokenizer_config.json', 'pytorch_model.bin', 'README (2).md', 'vocab.txt']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, BertTokenizer, pipeline\n\n# Specify the path where the model files are located\nmodel_path = \"/kaggle/input/pretrained-filess/\"  # Path to the folder containing the model files\n\n# Load the model and tokenizer from the local directory\nmodel = BertForSequenceClassification.from_pretrained(model_path)\ntokenizer = BertTokenizer.from_pretrained(model_path)\n\n# Specify the device (use 0 for GPU or -1 for CPU)\ndevice = 0  # Set to 0 for GPU, -1 for CPU\n\n# Create a text classification pipeline with the specified device\nclassifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=device)\n\n# Example SMS messages\nsms = [\n    \"Congratulations! You've won a $1,000 Walmart gift card. Go to http://spam.com to claim now.\",\n    \"Hi, are we still on for dinner tomorrow night?\"\n]\n\n# Label mapping\nlabel_map = {\n    \"LABEL_0\": \"spam\",\n    \"LABEL_1\": \"ham\"\n}\n\n# Get predictions\npredictions = classifier(sms)\n\n# Print predictions with human-readable labels\nfor i, message in enumerate(sms):\n    predicted_label = label_map[predictions[i][\"label\"]]\n    score = predictions[i][\"score\"]\n    print(f\"Message: {message}\")\n    print(f\"Prediction: {predicted_label} (confidence: {score:.2f})\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:10:08.619340Z","iopub.execute_input":"2024-12-17T05:10:08.620215Z","iopub.status.idle":"2024-12-17T05:10:26.605669Z","shell.execute_reply.started":"2024-12-17T05:10:08.620183Z","shell.execute_reply":"2024-12-17T05:10:26.604828Z"}},"outputs":[{"name":"stdout","text":"Message: Congratulations! You've won a $1,000 Walmart gift card. Go to http://spam.com to claim now.\nPrediction: spam (confidence: 0.79)\n\nMessage: Hi, are we still on for dinner tomorrow night?\nPrediction: spam (confidence: 0.94)\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install transformers datasets","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.47.0)\nCollecting dataset\n  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nCollecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: alembic>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.13.3)\nCollecting banal>=1.0.1 (from dataset)\n  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (4.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=0.6.2->dataset) (2.1.5)\nDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: banal, sqlalchemy, dataset\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 2.0.30\n    Uninstalling SQLAlchemy-2.0.30:\n      Successfully uninstalled SQLAlchemy-2.0.30\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.54\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n\n# 1. Load Dataset\ndata_path = \"/kaggle/input/cleaned/cleaned_Spam_SMS.csv\"\ndf = pd.read_csv(data_path)\n\n# Preprocess dataset\ntexts = df[\"Message\"].tolist()\nlabels = [1 if label == \"spam\" else 0 for label in df[\"Class\"].tolist()]  # Spam = 1, Ham = 0\n\n# Split into train and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    texts, labels, test_size=0.2, random_state=42\n)\n\n# 2. Tokenize Dataset\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef preprocess_function(texts):\n    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_encodings = preprocess_function(train_texts)\nval_encodings = preprocess_function(val_texts)\n\n# 3. Create Dataset Class\nclass SpamDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n# Create datasets\ntrain_dataset = SpamDataset(train_encodings, train_labels)\nval_dataset = SpamDataset(val_encodings, val_labels)\n\n# 4. Load Pre-trained BERT Model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# 5. Define Evaluation Metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1).numpy()\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n    acc = accuracy_score(labels, predictions)\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# 6. Define Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",           # Directory to save model checkpoints\n    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n    save_strategy=\"epoch\",           # Save model after each epoch\n    learning_rate=5e-5,              # Learning rate\n    per_device_train_batch_size=16,  # Batch size for training\n    per_device_eval_batch_size=16,   # Batch size for evaluation\n    num_train_epochs=3,              # Number of epochs\n    weight_decay=0.01,               # Weight decay for regularization\n    logging_dir=\"./logs\",            # Directory to save logs\n    logging_steps=10,                # Log every 10 steps\n    load_best_model_at_end=True,     # Load best model at the end of training\n    metric_for_best_model=\"f1\",      # Metric to determine the best model\n    save_total_limit=2,              # Keep only 2 checkpoints\n    report_to=\"none\",                # Disable W&B or other integrations\n)\n\n# 7. Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# 8. Train and Save the Model\ntrainer.train()\ntrainer.save_model(\"./spam_classifier_model\")\ntokenizer.save_pretrained(\"./spam_classifier_model\")\n\n# 9. Evaluate Model\nmetrics = trainer.evaluate()\nprint(\"Evaluation Metrics:\", metrics)\n\n# 10. Test the Model on New Examples\n# 10. Test the Model on New Examples\ntest_sms = [\n    \"Congratulations! You've won a $1,000 Walmart gift card. Go to http://spam.com to claim now.\",\n    \"Hi, are we still on for dinner tomorrow night?\",\n]\n\n# Tokenize the test messages\ntest_encodings = tokenizer(test_sms, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n\n# Move the encodings and model to the same device (GPU or CPU)\ndevice = model.device  # Get the device of the model (GPU or CPU)\ntest_encodings = {key: val.to(device) for key, val in test_encodings.items()}  # Move inputs to the same device\n\n# Make predictions\noutput = model(**test_encodings)\npredictions = torch.argmax(output.logits, dim=-1).tolist()\n\n# Map predictions to labels\nlabel_map = {0: \"ham\", 1: \"spam\"}\nfor i, sms in enumerate(test_sms):\n    print(f\"Message: {sms}\")\n    print(f\"Prediction: {label_map[predictions[i]]}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:11:00.272962Z","iopub.execute_input":"2024-12-17T05:11:00.273530Z","iopub.status.idle":"2024-12-17T05:14:58.191540Z","shell.execute_reply.started":"2024-12-17T05:11:00.273501Z","shell.execute_reply":"2024-12-17T05:14:58.190653Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"488edef556f4408788337ecb0d336310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93592d75900945ff8674efc9deac94b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c50fdbd54654ebda53d5bacf0bfd796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2b791d92e045538d527c4157afe719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b2baf9a4b7492b9c5fb9fe982ff929"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [420/420 03:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.048000</td>\n      <td>0.031899</td>\n      <td>0.994619</td>\n      <td>0.987421</td>\n      <td>0.975155</td>\n      <td>0.981250</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.026700</td>\n      <td>0.017980</td>\n      <td>0.997309</td>\n      <td>0.993750</td>\n      <td>0.987578</td>\n      <td>0.990654</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000500</td>\n      <td>0.016966</td>\n      <td>0.997309</td>\n      <td>0.993750</td>\n      <td>0.987578</td>\n      <td>0.990654</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [35/35 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Metrics: {'eval_loss': 0.017980312928557396, 'eval_accuracy': 0.9973094170403587, 'eval_precision': 0.99375, 'eval_recall': 0.9875776397515528, 'eval_f1': 0.9906542056074766, 'eval_runtime': 5.8201, 'eval_samples_per_second': 191.578, 'eval_steps_per_second': 6.014, 'epoch': 3.0}\nMessage: Congratulations! You've won a $1,000 Walmart gift card. Go to http://spam.com to claim now.\nPrediction: spam\nMessage: Hi, are we still on for dinner tomorrow night?\nPrediction: ham\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the pre-trained model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"./spam_classifier_model\")\ntokenizer = BertTokenizer.from_pretrained(\"./spam_classifier_model\")\n\n# Function to make a prediction on a custom input\ndef predict_spam(input_texts):\n    # Tokenize the input text\n    encodings = tokenizer(input_texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    \n    # Run the model on the input text\n    with torch.no_grad():\n        outputs = model(**encodings)\n    \n    # Get the predictions\n    predictions = torch.argmax(outputs.logits, dim=-1).tolist()\n    \n    # Map predictions to labels\n    label_map = {0: \"ham\", 1: \"spam\"}\n    for i, text in enumerate(input_texts):\n        print(f\"Message: {text}\")\n        print(f\"Prediction: {label_map[predictions[i]]}\")\n\n# Example usage with custom input\ncustom_texts = [\n    \"You've won a free vacation! Claim your prize now.\",\n    \"Congratulation on your win today at the match\"\n]\n\npredict_spam(custom_texts)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T05:10:26.597216Z","iopub.execute_input":"2024-12-16T05:10:26.597603Z","iopub.status.idle":"2024-12-16T05:10:27.089026Z","shell.execute_reply.started":"2024-12-16T05:10:26.597572Z","shell.execute_reply":"2024-12-16T05:10:27.087975Z"}},"outputs":[{"name":"stdout","text":"Message: You've won a free vacation! Claim your prize now.\nPrediction: ham\nMessage: Congratulation on your win today at the match\nPrediction: ham\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install gradio transformers torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:11.789085Z","iopub.execute_input":"2024-12-17T05:15:11.789811Z","iopub.status.idle":"2024-12-17T05:15:25.544956Z","shell.execute_reply.started":"2024-12-17T05:15:11.789752Z","shell.execute_reply":"2024-12-17T05:15:25.543857Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.4.0)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting gradio-client==1.5.2 (from gradio)\n  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.2)\nRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.9.2)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.13.2)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.2->gradio) (2024.6.1)\nRequirement already satisfied: websockets<15.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.2->gradio) (12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\n  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->gradio) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.23.4)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\nInstalling collected packages: semantic-version, ruff, python-multipart, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\n  Attempting uninstall: python-multipart\n    Found existing installation: python-multipart 0.0.9\n    Uninstalling python-multipart-0.0.9:\n      Successfully uninstalled python-multipart-0.0.9\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.37.2\n    Uninstalling starlette-0.37.2:\n      Successfully uninstalled starlette-0.37.2\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.111.0\n    Uninstalling fastapi-0.111.0:\n      Successfully uninstalled fastapi-0.111.0\nSuccessfully installed fastapi-0.115.6 ffmpy-0.4.0 gradio-5.9.1 gradio-client-1.5.2 python-multipart-0.0.20 ruff-0.8.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import gradio as gr\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the pre-trained model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained(\"./spam_classifier_model\")\ntokenizer = BertTokenizer.from_pretrained(\"./spam_classifier_model\")\n\n# Define the prediction function\ndef classify_spam(input_text):\n    # Tokenize the input text\n    encodings = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    \n    # Run the model on the input text\n    with torch.no_grad():\n        outputs = model(**encodings)\n    \n    # Get the predictions and confidence (softmax output)\n    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    confidence, prediction = torch.max(probabilities, dim=-1)\n    \n    # Map predictions to labels\n    label_map = {0: \"Not spam\", 1: \"Spam\"}\n    result = label_map[prediction.item()]\n    \n    # Return result with confidence percentage\n    return f\"{result} (Confidence: {confidence.item() * 100:.2f}%)\"\n\n# Create Gradio interface\niface = gr.Interface(\n    fn=classify_spam,  # Function to be called\n    inputs=gr.Textbox(label=\"Enter your message\"),  # Input component for user to type message\n    outputs=gr.Textbox(label=\"Prediction Result\"),  # Output component to show prediction\n    live=True,  # Update output in real-time\n    title=\"Spam Message Classifier\",  # Title of the app\n    description=\"Enter a message to check if it's spam or not along with the confidence score.\",  # Description\n)\n\n# Launch the app\niface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:28.030634Z","iopub.execute_input":"2024-12-17T05:15:28.031650Z","iopub.status.idle":"2024-12-17T05:15:32.332076Z","shell.execute_reply.started":"2024-12-17T05:15:28.031600Z","shell.execute_reply":"2024-12-17T05:15:32.331219Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://b7bbcfd941cc4907f8.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://b7bbcfd941cc4907f8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}